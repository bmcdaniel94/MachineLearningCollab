{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Benjamin McDaniel Q3 Exam 1","provenance":[{"file_id":"1Ffg7SnZoT66uv_uAvNKedhBAHZJM9t0s","timestamp":1583209935569}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"Xcxnac3eaBab","colab_type":"code","outputId":"ef18fa8f-7487-4ff7-b599-6d64524f3ac9","executionInfo":{"status":"error","timestamp":1583515375832,"user_tz":360,"elapsed":579,"user":{"displayName":"Benjamin McDaniel","photoUrl":"","userId":"17960085804568781582"}},"colab":{"base_uri":"https://localhost:8080/","height":248}},"source":["import os\n","import warnings\n","import numpy as np\n","from skimage import io\n","import skimage.feature as ft\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","from  google.colab  import  drive\n","import math\n","drive.mount('/content/gdrive')\n","mydirectory = 'gdrive/My Drive/Colab Notebooks/texture2/'\n","\n","def makedata(Nclass, W=40, Nrep=1):\n","    dataset=[]         #Each row corresponds to a window and contains LBP features of the window\n","    dataset2=[]        #Each row contains W**2 original pixels (flattened for standard ML tools)\n","    dataset3=[]        #WxW pixels (good for CNN networks)\n","    target=[]          #class labels\n","    dataset4 = []\n","\n","    # settings for LBP\n","    METHOD = 'uniform'\n","    P = 8\n","    R = 1\n","    P2 = 16\n","    R2 = 2\n","\n","    #Create dataset\n","    S=512    \n","    maxpos=S-W\n","    for ii in range(Nrep):       #Create a random permutation of 13 classes of textures Nrep times\n","        for i in np.random.permutation(13):\n","            filename = '1.1.'+str(i+1).zfill(2)+'.tiff'\n","            img = io.imread(mydirectory+filename)\n","            for j in range(Nclass):\n","                left=np.random.randint(0,maxpos+1)\n","                top=np.random.randint(0,maxpos+1)\n","                win=img[left:left+W,top:top+W]\n","\n","                #LBP features\n","                with warnings.catch_warnings():\n","                    warnings.simplefilter(\"ignore\")\n","                    feats=ft.local_binary_pattern(win, P, R, METHOD)\n","                    hist, _ = np.histogram(feats[R:-R][R:-R], normed=True, bins=P + 2, range=(0, P + 2))\n","                    feats2=ft.local_binary_pattern(win, P2, R2, METHOD)\n","                    hist2, _ = np.histogram(feats2[R2:-R2][R2:-R2], normed=True, bins=P2 + 2, range=(0, P2 + 2))\n","\n","                #Append the features of the window to the dataset\n","                dataset1.append(np.concatenate((hist, hist2)))\n","                dataset2.append(win.flatten())\n","                dataset3.append(win)\n","                \n","                # 0-255 rgb / 2 = 128 just enough to show some variation\n","                dataset4.append(win-128)\n","                \n","                #Window is coming from image i, so save it as the class label\n","                target.append(i+1)\n","                \n","                \n","    dataset=np.array(dataset)    \n","    target=np.array(target)    \n","    return dataset, dataset2, dataset3, dataset4, target\n","\n","\n","#Number of windows per texture\n","nclas=10\n","\n","W=30\n","dataset, dataset2, dataset3, dataset4, target = makedata(60+nclas,W,1)\n","dataset = np.insert(dataset, 0, 1, axis=1)\n","target = (target>1)+0\n","target = np.array(target).reshape((-1,1))\n","train_x, test_x, train_y, test_y = train_test_split(dataset,target,train_size=nclas*2, random_state=315, stratify=target)\n","   \n","(n_samples, d) = train_x.shape\n","print(n_samples)\n","eta=0.0003\n","import math\n","def sigmoid(x):\n","  return 1 / (1 + math.exp(-x))\n","\n","rng = np.random.RandomState(1)\n","w = rng.randn(1, d)/100\n","batch_update = True\n","\n","for epoch in range(100000):        #How many times to go over the dataset and update the weights\n","  numcorr=0\n","  dw=np.zeros(w.shape)\n","  for t in range(n_samples):\n","    r=train_y[t]\n","    x=train_x[t,:]\n","    y=sigmoid(np.dot(x,w.T))\n","    if (y>0.5 and r==1) or (y<=0.5 and r==0) :\n","      numcorr=numcorr+1     #this calculates the training accuracy\n","    delta=r-y\n","    if batch_update == True :\n","      dw=dw+x*delta           #accumulate the dw computed for each sample so that at the end of epoch we can make a more robust update\n","    else :\n","      w=w+eta*x*delta      \n","  if batch_update == True :\n","    w=w+eta*dw/n_samples\n","print(numcorr)            #changing the weights with every epoch will improve this number\n","\n","test_numcorr=0\n","n_test = test_x.shape[0]\n","for t in range(n_test):\n","    r=test_y[t]\n","    x=test_x[t,:]\n","    y=sigmoid(np.dot(x,w.T))\n","    if (y>0.5 and r==1) or (y<=0.5 and r==0) :\n","        test_numcorr=test_numcorr+1     #this calculates the training accuracy\n","print(test_numcorr/n_test)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-75fb7777401f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnclas\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m315\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0003\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"]}]}]}